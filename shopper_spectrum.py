# -*- coding: utf-8 -*-
"""shopper_spectrum.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WBiCFIKzHvuQn-LDP1mHsORdZ0c-OK7O

PROJECT DISCRIPTION:
"""

# Setup & Import Libraries
# Basic imports
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import datetime as dt
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import warnings
warnings.filterwarnings("ignore")

"""Step 1:Dataset Collection and understanding

"""

# Step 1: Load Dataset
import pandas as pd

# Load from specified path in Colab
df = pd.read_csv("/content/online_retail.csv")

# Show basic info
print("Dataset shape:", df.shape)
df.info()

# Preview the first 5 rows
df.head()

"""Step 2:  Data Preprocessing:"""

# Step 2.1: Remove rows with missing CustomerID
df = df.dropna(subset=['CustomerID'])

# Step 2.2: Remove cancelled invoices (InvoiceNo starting with 'C')
df = df[~df['InvoiceNo'].astype(str).str.startswith('C')]

# Step 2.3: Remove rows where Quantity or UnitPrice is zero or negative
df = df[(df['Quantity'] > 0) & (df['UnitPrice'] > 0)]

# Step 2.4: Convert InvoiceDate to datetime format
df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])

# Step 2.5: Add a TotalPrice column
df['TotalPrice'] = df['Quantity'] * df['UnitPrice']

# Step 2.6: Drop duplicates if any
df = df.drop_duplicates()

# Step 2.7: Reset index (optional)
df.reset_index(drop=True, inplace=True)

"""Quick Checks After Cleaning"""

print("Cleaned dataset shape:", df.shape)
print("Missing values:\n", df.isnull().sum())
print("Date range:", df['InvoiceDate'].min(), "to", df['InvoiceDate'].max())
print("Unique Customers:", df['CustomerID'].nunique())
print("Unique Products:", df['StockCode'].nunique())

""" Step 3: Exploratory Data Analysis EDA"""

# Country-wise total transactions
country_tx = df.groupby('Country')['InvoiceNo'].nunique().sort_values(ascending=False)

# Plot
import matplotlib.pyplot as plt
plt.figure(figsize=(12,6))
country_tx.plot(kind='bar', color='skyblue')
plt.title('Total Transactions by Country')
plt.ylabel('Number of Transactions')
plt.xticks(rotation=90)
plt.show()

# Top 10 most sold products by quantity
top_products = df.groupby('Description')['Quantity'].sum().sort_values(ascending=False).head(10)

# Plot
top_products.plot(kind='barh', figsize=(10,6), color='green')
plt.title('Top 10 Best-Selling Products')
plt.xlabel('Total Quantity Sold')
plt.gca().invert_yaxis()
plt.show()

# Group by month
df['Month'] = df['InvoiceDate'].dt.to_period('M')
monthly_sales = df.groupby('Month')['TotalPrice'].sum()

monthly_sales.plot(figsize=(10,5), marker='o')
plt.title('Monthly Sales Trend')
plt.ylabel('Revenue (Â£)')
plt.xticks(rotation=45)
plt.grid(True)
plt.show()

# Monetary Distribution per Transaction & Customer
import seaborn as sns

# Transaction-level TotalPrice
plt.figure(figsize=(10,5))
sns.histplot(df['TotalPrice'], bins=100, kde=True)
plt.title("Monetary Value per Transaction")
plt.xlim(0, 100)  # zoom in for readability
plt.show()

# Customer-level Total Monetary Spend
cust_spend = df.groupby('CustomerID')['TotalPrice'].sum()
plt.figure(figsize=(10,5))
sns.histplot(cust_spend, bins=100, kde=True)
plt.title("Total Monetary Spend per Customer")
plt.xlim(0, 2000)
plt.show()

#  RFM Value Distributions
# Assuming RFM DataFrame already exists from Step 2
# Snapshot date is the day after the last invoice date
snapshot_date = df['InvoiceDate'].max() + pd.Timedelta(days=1)

# Calculate RFM metrics
rfm = df.groupby('CustomerID').agg({
    'InvoiceDate': lambda x: (snapshot_date - x.max()).days,   # Recency
    'InvoiceNo': 'nunique',                                    # Frequency
    'TotalPrice': 'sum'                                        # Monetary
})

rfm.columns = ['Recency', 'Frequency', 'Monetary']

# Filter out zero monetary customers (just in case)
rfm = rfm[rfm['Monetary'] > 0]

# Preview
rfm.head()

fig, axes = plt.subplots(1, 3, figsize=(18, 5))

sns.histplot(rfm['Recency'], bins=30, ax=axes[0], color='purple')
axes[0].set_title("Recency Distribution")

sns.histplot(rfm['Frequency'], bins=30, ax=axes[1], color='orange')
axes[1].set_title("Frequency Distribution")

sns.histplot(rfm['Monetary'], bins=30, ax=axes[2], color='green')
axes[2].set_title("Monetary Distribution")

plt.tight_layout()
plt.show()

"""Elbow Curve for K-Means Cluster Selection"""

from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# Standardize
scaler = StandardScaler()
rfm_scaled = scaler.fit_transform(rfm)

# Elbow Method
sse = []
K_range = range(1, 11)
for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(rfm_scaled)
    sse.append(kmeans.inertia_)

plt.figure(figsize=(8,5))
plt.plot(K_range, sse, marker='o')
plt.title('Elbow Method for Optimal Clusters')
plt.xlabel('Number of Clusters')
plt.ylabel('SSE (Inertia)')
plt.grid(True)
plt.show()

"""Customer Cluster Profiles"""

# After applying KMeans
rfm['Cluster'] = KMeans(n_clusters=4, random_state=42).fit_predict(rfm_scaled)

# Cluster characteristics
rfm.groupby('Cluster').mean().round(2)

"""Product Recommendation Heatmap / Similarity Matrix"""

# Create item-user matrix
item_user = df.pivot_table(index='Description', columns='CustomerID', values='Quantity', fill_value=0)

# Compute product-product similarity (cosine)
from sklearn.metrics.pairwise import cosine_similarity
product_sim = cosine_similarity(item_user)

# Convert to DataFrame for labeling
sim_df = pd.DataFrame(product_sim, index=item_user.index, columns=item_user.index)

# Heatmap of top 10 products only (for readability)
import seaborn as sns
top10 = item_user.sum(axis=1).sort_values(ascending=False).head(10).index
sns.heatmap(sim_df.loc[top10, top10], cmap='coolwarm', annot=True, fmt=".2f")
plt.title("Product Similarity Heatmap (Top 10)")
plt.show()

"""Step 4 :  Clustering Methodology:"""

#  Feature Engineering â€” RFM Calculation

# RFM already calculated as:
# Recency: Days since last purchase
# Frequency: Number of invoices
# Monetary: Total spend
# Columns already renamed to ['Recency', 'Frequency', 'Monetary']

#  Standardize/Normalize the RFM Values

from sklearn.preprocessing import StandardScaler

# Select only the RFM columns, excluding 'Cluster' if already added
rfm_features = rfm[['Recency', 'Frequency', 'Monetary']]

# Scale only these 3 features
scaler = StandardScaler()
rfm_scaled = scaler.fit_transform(rfm_features)

# Convert back to DataFrame
rfm_scaled_df = pd.DataFrame(rfm_scaled, columns=['Recency', 'Frequency', 'Monetary'], index=rfm.index)

"""Choose Clustering Algorithm (Start with KMeans)"""

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

# Elbow Method
sse = []
silhouette_scores = []
K = range(2, 10)

for k in K:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(rfm_scaled)
    sse.append(kmeans.inertia_)
    silhouette_scores.append(silhouette_score(rfm_scaled, kmeans.labels_))

# Plot Elbow Curve
import matplotlib.pyplot as plt
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(K, sse, marker='o')
plt.title('Elbow Method')
plt.xlabel('k')
plt.ylabel('SSE')

# Plot Silhouette Score
plt.subplot(1, 2, 2)
plt.plot(K, silhouette_scores, marker='s', color='orange')
plt.title('Silhouette Score')
plt.xlabel('k')
plt.ylabel('Score')
plt.tight_layout()
plt.show()

"""Run KMeans Clustering"""

# Apply KMeans
optimal_k = 4  # based on elbow/silhouette
kmeans = KMeans(n_clusters=optimal_k, random_state=42)
rfm['Cluster'] = kmeans.fit_predict(rfm_scaled)

""" Label Clusters by RFM Averages"""

# View average RFM values per cluster
cluster_profiles = rfm.groupby('Cluster').mean().round(2)
cluster_profiles

""" Visualize the Clusters (2D/3D Plot)"""

import seaborn as sns

# 2D plot: Recency vs Frequency
plt.figure(figsize=(8,6))
sns.scatterplot(data=rfm, x='Recency', y='Frequency', hue='Cluster', palette='Set2')
plt.title("Customer Segments (Recency vs Frequency)")
plt.show()

# 3D plot (optional)
from mpl_toolkits.mplot3d import Axes3D

fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')
sc = ax.scatter(rfm['Recency'], rfm['Frequency'], rfm['Monetary'], c=rfm['Cluster'], cmap='tab10')
ax.set_xlabel('Recency')
ax.set_ylabel('Frequency')
ax.set_zlabel('Monetary')
plt.title("3D View of Customer Clusters")
plt.colorbar(sc)
plt.show()

""" Save the Clustering Model (for Streamlit Use)"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans

# Load dataset again (if needed)
df = pd.read_csv('/content/online_retail.csv')

# Preprocessing
df = df.dropna(subset=['CustomerID'])
df = df[~df['InvoiceNo'].astype(str).str.startswith('C')]
df = df[(df['Quantity'] > 0) & (df['UnitPrice'] > 0)]

# Convert date
df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])

# Add TotalPrice column
df['TotalPrice'] = df['Quantity'] * df['UnitPrice']

# Reference date
ref_date = df['InvoiceDate'].max() + pd.Timedelta(days=1)

# RFM computation
rfm = df.groupby('CustomerID').agg({
    'InvoiceDate': lambda x: (ref_date - x.max()).days,
    'InvoiceNo': 'nunique',
    'TotalPrice': 'sum'
})
rfm.columns = ['Recency', 'Frequency', 'Monetary']
rfm = rfm.reset_index()

scaler = StandardScaler()
rfm_scaled = scaler.fit_transform(rfm[['Recency', 'Frequency', 'Monetary']])

kmeans = KMeans(n_clusters=4, random_state=42)
kmeans.fit(rfm_scaled)

# Assign cluster labels
rfm['Cluster'] = kmeans.labels_

import joblib

joblib.dump(kmeans, '/content/kmeans_model.pkl')
joblib.dump(scaler, '/content/scaler.pkl')
rfm.to_csv('/content/rfm_with_clusters.csv', index=False)

"""Streamlit App Features and Streamlit App: Product Recommender"""

!pip install streamlit pyngrok --quiet

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# 
# st.title("ğŸ›’ Shopper Spectrum Demo")
# st.markdown("This is a test app running via Streamlit in Google Colab.")
# 
# st.text_input("Enter product name")
# st.button("Recommend")
#

# Install localtunnel and Streamlit
!pip install streamlit
!npm install -g localtunnel

# Run your Streamlit app in the background
!streamlit run app.py &

# Expose port 8501 using localtunnel
!npx localtunnel --port 8501

import streamlit as st
import pandas as pd
import numpy as np
import joblib

# Load models and data
kmeans = joblib.load('kmeans_model.pkl')
scaler = joblib.load('scaler.pkl')
similarity = joblib.load('product_similarity.pkl')
product_data = pd.read_csv('rfm_with_clusters.csv')  # Should include 'Description' column

# Set app layout
st.set_page_config(page_title="Shopper Spectrum", layout="centered")

# Sidebar navigation
st.sidebar.title("ğŸ§­ Navigation")
module = st.sidebar.radio("Choose a module:", ["ğŸ“¦ Product Recommendation", "ğŸ‘¤ Customer Segmentation"])

# =============================
# ğŸ“¦ Product Recommendation
# =============================
if module == "ğŸ“¦ Product Recommendation":
    st.title("ğŸ” Product Recommendation")
    st.write("Enter a product name and get 5 similar product suggestions!")

    product_name = st.text_input("ğŸ“¥ Enter Product Name")

    if st.button("Get Recommendations"):
        product_name = product_name.strip().lower()
        product_list = [p.lower() for p in similarity.index]

        if product_name in product_list:
            idx = similarity.index.tolist().index(product_name)
            sim_scores = list(enumerate(similarity.iloc[idx]))
            sim_scores = sorted(sim_scores, key=la

# =============================
# ğŸ‘¤ Customer Segmentation
# =============================
elif module == "ğŸ‘¤ Customer Segmentation":
    st.title("ğŸ‘¤ Customer Segmentation")
    st.write("Input Recency, Frequency, and Monetary values to find your customer segment.")

    # User Inputs
    recency = st.number_input("ğŸ“… Recency (days since last purchase)", min_value=0, step=1)
    frequency = st.number_input("ğŸ” Frequency (number of purchases)", min_value=0, step=1)
    monetary = st.number_input("ğŸ’° Monetary (total spend)", min_value=0.0, step=1.0)

    if st.button("ğŸš€ Predict Cluster"):
        input_data = np.array([[recency, frequency, monetary]])
        scaled_data = scaler.transform(input_data)
        cluster = kmeans.predict(scaled_data)[0]

        # Label Mapping (customize as per your analysis)
        cluster_labels = {
            0: "ğŸ§Š At-Risk Customer",
            1: "ğŸ’ High-Value Customer",
            2: "ğŸ†• New/Low Engagement",
            3: "ğŸ›’ Regular Buyer",
            4: "ğŸ“¦ Occasional Spender"
        }

        st.success(f"ğŸ¯ Predicted Segment: **Cluster {cluster}** â€” {cluster_labels.get(cluster, 'Unknown Segment')}")